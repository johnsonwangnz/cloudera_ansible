---
# Site Configuration
# ==================

- hosts: cluster 
  user: ansibler
  tasks:
    - name: determinei network interface
      set_fact: ipv4_address="{{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }}"
      # eg. to use a eth1: ipv4_address="{{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }}"

# cluster cross cutting components

- hosts: cluster
  user: ansibler
  sudo: true
  roles:
    - common
    # ganglia_monitor is not working at moment
    #- ganglia_monitor


- hosts: all
  user: ansibler
  sudo: true
  roles:
    - { role: oracle_jdk, when: jdk_installed is not defined }



- hosts: elasticsearch
  user: ansibler
  sudo: true
  roles:
    - elasticsearch
    #- elasticsearch_curator

- hosts: kibana
  user: ansibler
  sudo: true
  roles:
    - kibana

- hosts: logstash
  user: ansibler
  sudo: true
  roles:
    - logstash


- hosts: all
  user: ansibler
  sudo: true
  roles:
    - td_agent

# Hadoop

# Add cdh repo and key

- hosts: hdnodes
  user: ansibler
  sudo: true
  roles:
    - cdh_common


- hosts: zookeepers
  user: ansibler
  sudo: true
  roles:
    - cdh_zookeeper

- hosts: journalnodes
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_hdfs_journalnode

- hosts: resourcemanager
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_yarn_resourcemanager

- hosts: namenodes
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_hdfs_namenode
    - cdh_hadoop_hdfs_zkfc

- hosts: nodemanagers
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_yarn_nodemanager


- hosts: datanodes
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_hdfs_datanode

- hosts: nodemanagers
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_mapreduce

- hosts: historyserver
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_mapreduce_historyserver
    - cdh_hadoop_yarn_proxyserver

# config all hadoop components 
- hosts: hdnodes
  user: ansibler
  sudo: true
  roles:
    - cdh_hadoop_config

# create /data folder and format name node 

- hosts: primarynamenode
  user: ansibler
  sudo: true
  sudo_user: hdfs
  tasks:
    - name: create the /data/dfs/nn directory
      file: path=/data/dfs/nn owner=hdfs group=hdfs state=directory
      tags:
        - hadoop
        - hbase
        - hive
    - name: format the namenode - WILL NOT FORMAT IF /data/dfs/nn/current/VERSION EXISTS TO AVOID DATA LOSS
      command: creates=/data/dfs/nn/current/VERSION hdfs namenode -format -force
      tags:
        - hadoop
        - hbase
        - hive
#    - name: restart namenode 
#      service: name=hadoop-hdfs-namenode state=restarted
#      tags:
#        - hadoop
#        - hbase
#        - hive


# create /data folder on second namenode and format it with bootstrapStandby

- hosts: backupnamenode
  user: ansibler
  sudo: true
  sudo_user: hdfs
  tasks:
    - name: output the first namenode address
      debug: msg={{ hostvars[groups['primarynamenode'][0]].ipv4_address|default(hostvars[groups['primarynamenode'][0]].ansible_default_ipv4.address) }} 

    - name: wait for the first namenode to come online
      wait_for: host={{ hostvars[groups['primarynamenode'][0]].ipv4_address|default(hostvars[groups['primarynamenode'][0]].ansible_default_ipv4.address) }} port=50070
      tags:
        - hadoop
        - hbase
        - hive
    - name: create the /data/dfs/nn directory
      file: path=/data/dfs/nn owner=hdfs group=hdfs state=directory
      tags:
        - hadoop
        - hbase
        - hive
    - name: bootstrap the standby namenode
      shell: creates=/data/dfs/nn/.bootstrapped hdfs namenode -bootstrapStandby && touch /data/dfs/nn/.bootstrapped
      tags:
        - hadoop
        - hbase
        - hive

# primary namenode creates zookeeper folder, start failover controller and restart namenodes

- hosts: primarynamenode
  user: ansibler
  sudo: true
  tasks:
    - name: format hadoop-hdfs-zkfc
      shell: creates=/data/dfs/.zkfsformatted hdfs zkfc -formatZK -force && touch /data/dfs/.zkfsformatted
      tags:
        - hadoop
        - hbase
        - hive
    - name: start zkfc
      service: name=hadoop-hdfs-zkfc state=restarted
      tags:
        - hadoop
        - hbase
        - hive
    - name: restart namenode
      service: name=hadoop-hdfs-namenode state=restarted
      tags:
        - hadoop
        - hbase
        - hive

# restart all services

- hosts: journalnodes
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-hdfs-journalnode on journalnodes
      service: name=hadoop-hdfs-journalnode state=restarted
  tags:
    - restartallservices

- hosts: primarynamenode
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-hdfs-namenode on primary namenode
      service: name=hadoop-hdfs-namenode state=restarted
    - name: restart zkfc on primary namenode 
      service: name=hadoop-hdfs-zkfc state=restarted
  tags:
    - restartallservices

- hosts: backupnamenode
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-hdfs-namenode on backup namenode
      service: name=hadoop-hdfs-namenode state=restarted
    - name: restart zkfc on backup namenode
      service: name=hadoop-hdfs-zkfc state=restarted
  tags:
    - restartallservices

- hosts: datanodes
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-hdfs-datanode on all datanodes
      service: name=hadoop-hdfs-datanode state=restarted
  tags:
    - restartallservices

- hosts: historyserver
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-mapreduce-historyserver on history server
      service: name=hadoop-mapreduce-historyserver state=restarted
  tags:
    - restartallservices

- hosts: resourcemanager
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-yarn-resourcemanager on resourcemanager
      service: name=hadoop-yarn-resourcemanager state=restarted
  tags:
    - restartallservices


- hosts: nodemanagers
  user: ansibler
  sudo: true
  tasks:
    - name: restart hadoop-yarn-nodemanager on all node managers
      service: name=hadoop-yarn-nodemanager state=restarted
  tags:
    - restartallservices


- hosts: datanodes
  user: hdfs
  tasks:
    - name: refresh datanodes
      command: hdfs dfsadmin -refreshNodes
  tags:
    - restartallservices

