---
# Site Configuration
# ==================

- hosts: cluster 
  user: ansibler
  tasks:
    - name: determinei network interface
      set_fact: ipv4_address="{{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }}"
      # eg. to use a eth1: ipv4_address="{{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }}"
      # all tags the uses ipv4_address will need to run this as well, otherwise ipv4 address might be using 10.0....
  tags:
    - ganglia
    - kibana

# cluster cross cutting components

- hosts: cluster
  user: ansibler
  become: true
  roles:
    - common


- hosts: all
  user: ansibler
  become: true
  roles:
    - { role: oracle_jdk, when: jdk_installed is not defined }



- hosts: elasticsearch
  user: ansibler
  become: true
  roles:
    - elasticsearch
    #- elasticsearch_curator

- hosts: kibana
  user: ansibler
  become: true
  roles:
    - kibana

- hosts: logstash
  user: ansibler
  become: true
  roles:
    - logstash


- hosts: all
  user: ansibler
  become: true
  roles:
    - td_agent

# install Ganglia
- hosts: gmserver
  user: ansibler
  become: true
  roles:
    - ganglia_web
    - ganglia_gmetad
  tags:
    - ganglia

- hosts: gmreceiver:gmsenders
  user: ansibler
  become: true
  roles:
    - ganglia_monitor
  tags:
    - ganglia

# install Prometheus
- hosts: prometheus
  user: ansibler
  become: true
  roles:
    - prometheus
  tags:
    - prometheus

# install Grafana
- hosts: grafana
  user: ansibler
  become: true
  roles:
    - grafana
  tags:
    - grafana



# Hadoop

# Add cdh repo and key

- hosts: hdnodes:hdclients
  user: ansibler
  become: true
  roles:
    - cdh_common


- hosts: zookeepers
  user: ansibler
  become: true
  roles:
    - cdh_zookeeper

- hosts: journalnodes
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_hdfs_journalnode

- hosts: resourcemanagers
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_yarn_resourcemanager

- hosts: namenodes
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_hdfs_namenode
    - cdh_hadoop_hdfs_zkfc

- hosts: nodemanagers
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_yarn_nodemanager


- hosts: datanodes
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_hdfs_datanode

- hosts: nodemanagers
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_mapreduce

- hosts: historyserver
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_mapreduce_historyserver
    - cdh_hadoop_yarn_proxyserver

- hosts: hdclients
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_client
 

# config all hadoop components 
- hosts: hdnodes:hdclients
  user: ansibler
  become: true
  roles:
    - cdh_hadoop_config

# create /data folder and format name node 

- hosts: primarynamenode
  user: ansibler
  become: true
  become_user: hdfs
  tasks:
    - name: create the /data/dfs/nn directory
      file: path=/data/dfs/nn owner=hdfs group=hdfs state=directory
      tags:
        - hadoop
        - hbase
        - hive
    - name: format the namenode - WILL NOT FORMAT IF /data/dfs/nn/current/VERSION EXISTS TO AVOID DATA LOSS
      command: creates=/data/dfs/nn/current/VERSION hdfs namenode -format -force
      tags:
        - hadoop
        - hbase
        - hive

- hosts: primarynamenode
  user: ansibler
  become: true
  tasks:
    - name: restart hadoop-hdfs-namenode when namenode was just formated for second namenode provision
      service: name=hadoop-hdfs-namenode state=restarted


# create /data folder on second namenode and format it with bootstrapStandby

- hosts: backupnamenode
  user: ansibler
  become: true
  become_user: hdfs
  tasks:
    - name: output the first namenode address
      debug: msg={{ hostvars[groups['primarynamenode'][0]].ipv4_address|default(hostvars[groups['primarynamenode'][0]].ansible_default_ipv4.address) }} 

    - name: wait for the first namenode to come online
      wait_for: host={{ hostvars[groups['primarynamenode'][0]].ipv4_address|default(hostvars[groups['primarynamenode'][0]].ansible_default_ipv4.address) }} port=50070
      tags:
        - hadoop
        - hbase
        - hive
    - name: create the /data/dfs/nn directory
      file: path=/data/dfs/nn owner=hdfs group=hdfs state=directory
      tags:
        - hadoop
        - hbase
        - hive
    - name: bootstrap the standby namenode
      shell: creates=/data/dfs/nn/.bootstrapped hdfs namenode -bootstrapStandby && touch /data/dfs/nn/.bootstrapped
      tags:
        - hadoop
        - hbase
        - hive

# primary namenode creates zookeeper folder, start failover controller and restart namenodes

- hosts: primarynamenode
  user: ansibler
  become: true
  tasks:
    - name: format hadoop-hdfs-zkfc
      shell: creates=/data/dfs/.zkfsformatted hdfs zkfc -formatZK -force && touch /data/dfs/.zkfsformatted
      tags:
        - hadoop
        - hbase
        - hive
    - name: start zkfc
      service: name=hadoop-hdfs-zkfc state=restarted
      tags:
        - hadoop
        - hbase
        - hive
    - name: restart namenode
      service: name=hadoop-hdfs-namenode state=restarted
      tags:
        - hadoop
        - hbase
        - hive

# Create spark nodes and spark history server
- hosts: hdnodes
  user: ansibler
  become: true
  roles:
    - cdh_spark

- hosts: sparkhistoryserver
  user: ansibler
  become: true
  roles:
    - cdh_spark_history_server



